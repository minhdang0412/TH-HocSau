{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIl0wHkuF7f1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZaT3u7_7G6eZ"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_afhlTp5G9xa",
        "outputId": "d4b884e7-8a48-4e89-d12a-77b216b72951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: \n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_ne_chunker_tab ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger_tab Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "Hit Enter to continue: \n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt_tab........... Punkt Tokenizer Models\n",
            "Hit Enter to continue: \n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "Hit Enter to continue: \n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets (JSON)\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "Hit Enter to continue: \n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [P] all................. All packages\n",
            "  [P] book................ Everything used in the NLTK Book\n",
            "  [P] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages; [P] marks partially installed collections)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "nltk.download_shell()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR1e3yB8HHOP",
        "outputId": "40871b04-5ff2-4fef-af0a-b61141e31b91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2jmUWW2H9UE",
        "outputId": "34acc6c3-2e66-469d-89a7-4881cd0de48e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print(\"Gutenberg files : \", gb.fileids())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SukuR-LbIDvk"
      },
      "outputs": [],
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_gkIyYYIF3Z",
        "outputId": "92cae86f-0fbf-4025-8bf1-f5ddf8dee885"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(macbeth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sr3xxLqIIf1",
        "outputId": "9a4fc365-2711-42f1-9630-0b97d07a6969"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "macbeth [:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMfyfqVHINHj",
        "outputId": "01b67380-b0d8-412f-dfc9-c6cf40a02887"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[['[',\n",
              "  'The',\n",
              "  'Tragedie',\n",
              "  'of',\n",
              "  'Macbeth',\n",
              "  'by',\n",
              "  'William',\n",
              "  'Shakespeare',\n",
              "  '1603',\n",
              "  ']'],\n",
              " ['Actus', 'Primus', '.'],\n",
              " ['Scoena', 'Prima', '.'],\n",
              " ['Thunder', 'and', 'Lightning', '.'],\n",
              " ['Enter', 'three', 'Witches', '.']]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijhpvue5JWhQ",
        "outputId": "e951fef1-0367-4bbe-a884-d5f11b914f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ]
        }
      ],
      "source": [
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_f48IV1JaB7",
        "outputId": "90178666-aa48-491d-adc1-61ef11cf3d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the_. bloody_: the_,\n"
          ]
        }
      ],
      "source": [
        "text.common_contexts(['Stage'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8axvAXUUJca2",
        "outputId": "845bb161-1d07-4384-d368-db79f351e680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day time face warre ayre king bleeding man reuolt serieant like\n",
            "knowledge broyle shew head spring heeles hare thane skie\n"
          ]
        }
      ],
      "source": [
        "text.similar('Stage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bDFZ8NcJyx1",
        "outputId": "cdd50fea-156d-48bc-970b-e38d4241f31d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fd = nltk.FreqDist(macbeth)\n",
        "fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jAaE3uKJ4Zr",
        "outputId": "5b184c65-60b1-4d81-ae7d-f28a9515e33a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enu_UM3ZJ9ye",
        "outputId": "89fcbfb9-9c46-42cd-99f7-36f014157b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "198\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['very',\n",
              " 'haven',\n",
              " 'now',\n",
              " 'above',\n",
              " 'she',\n",
              " 'am',\n",
              " 'are',\n",
              " \"shan't\",\n",
              " \"he's\",\n",
              " \"they'll\"]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJwoTxjRKAhS",
        "outputId": "575b41f5-b0d1-4e56-bb8a-27472211b9a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14946"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "len(macbeth_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnclq9jGKEKT",
        "outputId": "d81f7402-f9eb-4ca8-ff9b-4297a3198162"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrAYnHiHKHkW",
        "outputId": "80eaa2ff-0545-4f2e-c9ce-39c15669ca53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('macb', 137),\n",
              " ('haue', 122),\n",
              " ('thou', 90),\n",
              " ('enter', 81),\n",
              " ('shall', 68),\n",
              " ('macbeth', 62),\n",
              " ('vpon', 62),\n",
              " ('thee', 61),\n",
              " ('macd', 58),\n",
              " ('vs', 57)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "fd.most_common(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7eRtGJcKi0l",
        "outputId": "0a0bd7b4-bfe5-4f8d-8040-9fe018f5ed5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "long_words = [w for w in macbeth if len(w)> 12]\n",
        "sorted(long_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krY_eZAEKvYG",
        "outputId": "b2eb577b-6597-4e1a-b198-b5404c5c0d58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "sorted(ious_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtrETcLhKzjH",
        "outputId": "d1dbdbc4-6cb8-452e-9804-79c08434237e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLo5TDtBK3Mr",
        "outputId": "73e2fd2c-8f71-4ee0-f8cc-5310b9339c4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
        "tgrms.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R11KFDQRK9RC",
        "outputId": "11f864a7-954c-4b42-fc2b-8cfee27e9b33"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "639H1XQgLDYT",
        "outputId": "e4b9dd76-87a1-4d2b-d38d-990ab69b3f89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from urllib import request\n",
        "\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8')  # Sử dụng 'utf-8' thay vì 'utf8-sig'\n",
        "raw[:75]  # Hiển thị 75 ký tự đầu tiên"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-8RdsyaLPtn",
        "outputId": "e2a595ea-bd48-484b-eed4-372df59fe1e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['*',\n",
              " '*',\n",
              " '*',\n",
              " 'START',\n",
              " 'OF',\n",
              " 'THE',\n",
              " 'PROJECT',\n",
              " 'GUTENBERG',\n",
              " 'EBOOK',\n",
              " '2554',\n",
              " '*',\n",
              " '*']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize (raw)\n",
        "webtext = nltk.Text (tokens)\n",
        "webtext[:12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b_6WslGfLTeI",
        "outputId": "d6ebffeb-144a-4cfd-8623-8f2282c3e3d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:120]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qoIPvjD9LYhb"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAG0JWmvLbKb",
        "outputId": "954fa18f-6693-4526-f550-662f061b1ebf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cpPPecXyLeo3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category)\n",
        "for category in reviews.categories()\n",
        "for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H75rS5D4LlhU",
        "outputId": "3cb05c89-03a6-46ae-a132-7bf2a5a676d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "joe versus the volcano is really one of the worse movies made in very recent memory . the strangest thing is that you would think nothing would go wrong with it . it has a solid cast with tom hanks & meg ryan as the lead roles . but you can never judge a movie by its cast . . . if there is one good thing about joe vs the volcano , it is that the plot is original . unfortunately , it is also incredibly stupid . the movie begins with joe ( tom hanks ) going to work . this opening sequence is very boring and slow . it shows joe walking to his office . but on the way , he has to wait in a long line passing by strange and slightly depressing scenery with obnoxious lighting . a sequence like this should take 2 minutes . here , it takes over 5 . . . it is obvious that joe hates his job . at his office , one of his co - workers is meg ryan . oddly enough , she plays 3 different roles in this movie ! ! joe leaves to go to a doctor ' s appointment . his doctor informs him that he has a \" brain cloud . \" this means that in a few months , he will die . so what does joe do ? quit his job , of course ! ! when he arrives home , joe meets an old man named graynamore ( lloyd bridges ) . graynamore tells joe that in order to get some important mineral for his company from an island , the natives need someone to sacrifice to their volcano to please their fire god . by a startling coincidence , the boat trip to the island takes a few months . by the time he reaches the island , he will almost be dead from the \" brain cloud \" anyway , so joe agrees . graynamore gives joe a credit card to buy everything he needs to go on this \" great adventure \" . joe goes on a date with meg ryan , his co - worker . the day before the boat trip , joe meets one of graynamore ' s daughters . she is also , surprise , meg ryan . except she looks more hippyish . they both have dinner and the next day , joe gets driven over to his boat . the lady that sails the boat is another one of graynamore ' s daughters . and , wonder of wonders , she is also played by meg ryan . as they sail to the island , meg ryan tells joe that the natives of the island have a craving for orange soda . after some stupid talking scenes , they also deduce that graynamore had his doctor be the one that joe went to . and that the \" brain cloud \" thing was made up ! ! they do plently of cheap special effects on the boat voyage . joe goes fishing and catches a hammerhead shark . this is a cheap gag that has been pulled off many times . but , to top it off , the shark is obviously rubber and fake ! ! one night there is a storm . a cheap - looking lightning bolt strikes the boat and everything is cast overboard . fortunately , joe and meg ryan manage to find some of the luggage that joe brought to sail . by pure luck , they get to the island . since the natives like orange soda , they wear soda cans as attire ! ! stupid or what ? ; ) before joe leaps in the volcano , he gets fed . right before he jumps , meg ryan pleads him not to . when joe decides to , she goes in with him because \" she loves him \" . now this is where the movie should end . but , unfortunately , the \" cheesy ending \" bug comes in . the volcano blows the couple out and into the ocean where they land on joe ' s luggage . they float to another part of the island as they watch lava pouring out of the volcano towards the villagers . i give this movie a . see it only if you ' re a film buff that enjoys a bad movie every now and then or if you really like tom hanks or meg ryan . . .\n"
          ]
        }
      ],
      "source": [
        "first_review = ' '.join(documents[0][0])\n",
        "print(first_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y6bVnHjKLt81",
        "outputId": "5ec7dc26-32e9-45f3-9566-22049fc77d3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'neg'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "6_8wio6pLvz7"
      },
      "outputs": [],
      "source": [
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8cnX0p2oL5mq"
      },
      "outputs": [],
      "source": [
        "def document_features(document, word_features):\n",
        "    document_words = set(document)  # Chuyển văn bản thành một tập hợp các từ (set)\n",
        "    features = {}  # Khởi tạo từ điển để lưu các tính năng\n",
        "    for word in word_features:  # Duyệt qua các từ trong word_features\n",
        "        features['{}'.format(word)] = (word in document_words)  # Kiểm tra từ có trong document không\n",
        "    return features  # Trả về từ điển tính năng\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi-y-d9vMWQ4",
        "outputId": "56f0c8e9-f7d3-43a5-ca38-bf601e4238a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
        "len(featuresets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-At80bISMoy0"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GY3kNyUNDN_",
        "outputId": "79f9a22c-7629-413d-94fa-ed9f421a0fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.762\n"
          ]
        }
      ],
      "source": [
        "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4mSO99kN3QE",
        "outputId": "baba8b86-f5ed-4688-d1a2-a0bfae228f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                theaters = True              pos : neg    =     10.0 : 1.0\n",
            "              progresses = True              pos : neg    =      9.4 : 1.0\n",
            "                  reveal = True              pos : neg    =      9.4 : 1.0\n",
            "               community = True              pos : neg    =      8.7 : 1.0\n",
            "                     era = True              pos : neg    =      8.7 : 1.0\n",
            "             masterpiece = True              pos : neg    =      8.7 : 1.0\n",
            "               ludicrous = True              neg : pos    =      8.6 : 1.0\n",
            "               offensive = True              neg : pos    =      8.6 : 1.0\n",
            "               memorable = True              pos : neg    =      8.1 : 1.0\n",
            "                    grim = True              pos : neg    =      8.1 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0IVZBwiPjdA",
        "outputId": "e1b7257a-5505-4b79-84d5-0f4456385b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5YpmfryXPmTP",
        "outputId": "111ffeed-12c8-488b-951b-cefb21145503"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'nltk.corpus' has no attribute '__all__'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-0046c462d853>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Liệt kê tất cả corpus có sẵn trong NLTK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcorpus_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Danh sách các corpus có sẵn trong NLTK:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.corpus' has no attribute '__all__'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')  # Tải các corpus phổ biến từ NLTK nếu chưa tải\n",
        "from nltk.corpus import stopwords, names, brown, reuters\n",
        "\n",
        "# Liệt kê tất cả corpus có sẵn trong NLTK\n",
        "corpus_list = nltk.corpus.__all__\n",
        "\n",
        "print(\"Danh sách các corpus có sẵn trong NLTK:\")\n",
        "for corpus in corpus_list:\n",
        "    print(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRC2ltCMPth9",
        "outputId": "f7cb04c0-8ae2-4143-e5ab-c66dc1ca794b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Danh sách các stopwords cho các ngôn ngữ khác nhau:\n",
            "English: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']...\n",
            "French: ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']...\n",
            "German: ['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']...\n",
            "Spanish: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']...\n",
            "Portuguese: ['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']...\n",
            "Italian: ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']...\n",
            "Dutch: ['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij']...\n",
            "Russian: ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')  # Tải bộ stopwords từ NLTK\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Liệt kê stopwords cho các ngôn ngữ khác nhau\n",
        "languages = ['english', 'french', 'german', 'spanish', 'portuguese', 'italian', 'dutch', 'russian']\n",
        "\n",
        "print(\"Danh sách các stopwords cho các ngôn ngữ khác nhau:\")\n",
        "\n",
        "for language in languages:\n",
        "    try:\n",
        "        # Lấy stopwords cho ngôn ngữ\n",
        "        stop_words = stopwords.words(language)\n",
        "        print(f\"{language.capitalize()}: {stop_words[:10]}...\")  # In ra 10 từ stopword đầu tiên\n",
        "    except OSError:\n",
        "        print(f\"Không tìm thấy stopwords cho ngôn ngữ {language}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vftRZGd4RwAO",
        "outputId": "846a779a-d09f-4464-c342-39d292b60b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopwords cho ngôn ngữ English:\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
            "\n",
            "Stopwords cho ngôn ngữ French:\n",
            "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']\n",
            "\n",
            "Stopwords cho ngôn ngữ German:\n",
            "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']\n",
            "\n",
            "Stopwords cho ngôn ngữ Spanish:\n",
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n",
            "\n",
            "Stopwords cho ngôn ngữ Portuguese:\n",
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n",
            "\n",
            "Stopwords cho ngôn ngữ Italian:\n",
            "['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']\n",
            "\n",
            "Stopwords cho ngôn ngữ Dutch:\n",
            "['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij']\n",
            "\n",
            "Stopwords cho ngôn ngữ Russian:\n",
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']\n",
            "\n",
            "Stopwords cho ngôn ngữ Chinese:\n",
            "['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时']\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')  # Tải bộ stopwords từ NLTK\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Danh sách các ngôn ngữ bạn muốn kiểm tra stopwords\n",
        "languages = ['english', 'french', 'german', 'spanish', 'portuguese', 'italian', 'dutch', 'russian', 'chinese']\n",
        "\n",
        "# Kiểm tra và in ra stopwords cho mỗi ngôn ngữ\n",
        "for language in languages:\n",
        "    try:\n",
        "        # Lấy stopwords cho ngôn ngữ\n",
        "        stop_words = stopwords.words(language)\n",
        "        print(f\"Stopwords cho ngôn ngữ {language.capitalize()}:\")\n",
        "        print(stop_words[:10])  # In ra 10 từ stopwords đầu tiên\n",
        "        print()  # Thêm dòng trống cho dễ đọc\n",
        "    except OSError:\n",
        "        print(f\"Không tìm thấy stopwords cho ngôn ngữ {language.capitalize()}.\")\n",
        "        print()  # Thêm dòng trống cho dễ đọc\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
